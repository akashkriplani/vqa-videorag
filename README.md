# Enhancing Visual Question Answering Using Retrieval Augmented Generation in Medical Domain

A comprehensive Retrieval-Augmented Generation (RAG) system for answering medical questions from educational videos using state-of-the-art multimodal embeddings, hybrid search, and LLM-based answer generation.

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [System Architecture](#-system-architecture)
- [Hardware Requirements](#-hardware-requirements)
- [Installation](#-installation)
- [Quick Start](#-quick-start)
- [Detailed Usage Guide](#-detailed-usage-guide)
- [Project Structure](#-project-structure)
- [Configuration Options](#-configuration-options)
- [Example Workflows](#-example-workflows)
- [Performance Benchmarks](#-performance-benchmarks)
- [Troubleshooting](#-troubleshooting)

## ğŸ¯ Overview

This project implements a **Medical VideoRAG** system that processes medical educational videos to answer natural language questions. The system extracts both textual (speech) and visual information from videos, creates searchable embeddings, and generates accurate, evidence-based answers with proper source attribution.

**Key Innovation**: Combines dense semantic search (BiomedCLIP embeddings) with sparse lexical search (BM25) for optimal retrieval in the medical domain, enhanced with adaptive context curation and self-reflection attribution.

**Dataset**: Built on the [MedVidQA dataset](https://github.com/deepaknlp/MedVidQA) - 800+ medical YouTube videos with 3,000+ question-answer pairs.

## âœ¨ Key Features

### ğŸ” Multimodal Processing

- **Automatic Speech Recognition**: OpenAI Whisper (tiny) for fast, accurate medical transcription
- **Text Embeddings**: BiomedCLIP text encoder with sliding window segmentation (256 tokens, 192 stride)
- **Visual Embeddings**: BiomedCLIP vision encoder with adaptive frame sampling
- **Medical NER**: SciSpacy entity recognition for medical terminology extraction

### ğŸ” Advanced Search

- **Dense Search**: FAISS-based vector similarity (BiomedCLIP embeddings)
- **Sparse Search**: BM25 keyword matching with medical term expansion
- **Hybrid Fusion**: Configurable linear/RRF fusion (default: Î±=0.3, 70% BM25 + 30% dense)
- **Hierarchical Search**: Extended context retrieval with overlapping timestamps

### ğŸ§  Intelligent Answer Generation

- **Context Curation**: Multi-stage filtering with NLI factuality scoring
- **Query Classification**: Procedural, diagnostic, factoid, or general
- **Self-Reflection Attribution**: Claim-level evidence mapping
- **Cost-Optimized LLM**: GPT-4o-mini (~$0.0003-0.0005 per query)

### ğŸ“Š Comprehensive Evaluation

- **Retrieval Metrics**: Recall@K, MRR, temporal IoU/F1, video hit rate
- **Generation Metrics**: ROUGE-L, answer quality, confidence scores
- **Performance Timing**: End-to-end latency tracking
- **Cost Tracking**: Token usage and API cost estimation

## ğŸ—ï¸ System Architecture

The system operates in **6 phases**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: Data Understanding and Preprocessing                   â”‚
â”‚ â€¢ Download videos from YouTube                                  â”‚
â”‚ â€¢ Clean dataset (remove duplicates, validate timestamps)        â”‚
â”‚ â€¢ Split into train/val/test                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: Multimodal Feature Extraction                          â”‚
â”‚ â€¢ ASR: Extract audio â†’ Whisper â†’ Transcripts with timestamps    â”‚
â”‚ â€¢ Text: Sliding windows â†’ Medical NER â†’ BiomedCLIP embeddings   â”‚
â”‚ â€¢ Visual: Adaptive sampling â†’ Frame extraction â†’ CLIP embeddingsâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: Embedding Storage & Indexing                           â”‚
â”‚ â€¢ Normalize embeddings (L2 norm)                                â”‚
â”‚ â€¢ Build FAISS indices (separate text/visual, train/val/test)    â”‚
â”‚ â€¢ Persist metadata (JSON) with segment info & timestamps        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: Multimodal Search and Retrieval                        â”‚
â”‚ â€¢ Query embedding (BiomedCLIP)                                  â”‚
â”‚ â€¢ Parallel search: Dense (FAISS) + Sparse (BM25)                â”‚
â”‚ â€¢ Hybrid fusion (linear/RRF)                                    â”‚
â”‚ â€¢ Multimodal aggregation (text + visual by segment_id)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 5: Answer Generation with Attribution                     â”‚
â”‚ â€¢ Context curation (quality filtering, NLI scoring)             â”‚
â”‚ â€¢ Query classification (procedural/diagnostic/factoid/general)  â”‚
â”‚ â€¢ GPT-4o-mini generation (temperature=0.3, max_tokens=250)      â”‚
â”‚ â€¢ Self-reflection attribution (claimâ†’evidence mapping)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 6: Evaluation and Experimentation                         â”‚
â”‚ â€¢ Retrieval metrics: Recall@K, MRR, temporal IoU/F1             â”‚
â”‚ â€¢ Generation metrics: Answer quality, confidence                â”‚
â”‚ â€¢ Ablation studies: Dense vs hybrid, fusion strategies          â”‚
â”‚ â€¢ Hyperparameter optimization: Grid search, performance plots   â”‚
â”‚ â€¢ Cost analysis: Token usage, API costs, latency profiling      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![VideoRAG Architecture for Medical VQA](images/System_Architecture.png)

## ğŸ’» Hardware Requirements

### Minimum Requirements

- **CPU**: 4+ cores
- **RAM**: 16GB
- **Storage**: 50GB free (videos: ~30GB, features: ~10GB, indices: ~5GB)
- **GPU**: Optional (speeds up embedding generation 3-5x)

### Recommended Configuration

- **CPU**: 8+ cores (Intel i7/AMD Ryzen 7 or better)
- **RAM**: 32GB
- **GPU**: NVIDIA GPU with 8GB+ VRAM (RTX 3060 Ti or better) OR Apple M1/M2 with 16GB+ unified memory
- **Storage**: SSD with 100GB+ free space

### Performance Notes

- **Without GPU**: Full pipeline on 899 videos takes ~24-48 hours
- **With GPU (CUDA/MPS)**: ~6-12 hours
- **Parallel Processing**: Adjust `max_workers` in [multimodal_pipeline_with_sliding_window.py](multimodal_pipeline_with_sliding_window.py) based on your CPU/GPU

## ğŸš€ Installation

### 1. Clone Repository

```bash
git clone <repository-url>
cd <repository-directory>
```

### 2. Create Python Environment

```bash
# Using conda (recommended)
conda create -n medvidrag python=3.8
conda activate medvidrag

# OR using venv
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install System Dependencies

**macOS:**

```bash
brew install ffmpeg
npm install -g youtube-po-token-generator
```

**Ubuntu/Debian:**

```bash
sudo apt-get update
sudo apt-get install ffmpeg
npm install -g youtube-po-token-generator
```

**Windows:**

- Download FFmpeg from https://ffmpeg.org/download.html
- Add to PATH
- Install Node.js and run: `npm install -g youtube-po-token-generator`

### 4. Install Python Dependencies

**Standard Installation:**

```bash
pip install -r requirements.txt
```

**With Adaptive Context Curation:**

```bash
pip install -r requirements_adaptive.txt
```

**Install SciSpacy Model:**

```bash
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz
```

**Download NLTK Data:**

```bash
python -c "import nltk; nltk.download('punkt'); nltk.download('averaged_perceptron_tagger')"
```

### 5. GPU Setup (Optional but Recommended)

**For NVIDIA GPUs:**

```bash
# Uninstall CPU version
pip uninstall faiss-cpu

# Install GPU version
pip install faiss-gpu>=1.7.4
```

**For Apple Silicon (M1/M2/M3):**

- PyTorch automatically uses MPS (Metal Performance Shaders)
- No additional setup required

### 6. Configure Environment Variables

Create a `.env` file:

```bash
# OpenAI API key (required for answer generation)
OPENAI_API_KEY=your_openai_api_key_here

# HuggingFace token (for gated models like BiomedCLIP)
HF_TOKEN=your_huggingface_token_here
```

Get your API keys:

- OpenAI: https://platform.openai.com/api-keys
- HuggingFace: https://huggingface.co/settings/tokens

## ğŸ¬ Quick Start

### Option 1: Full Pipeline (Recommended for First-Time Users)

```bash
# Run everything: data prep â†’ feature extraction â†’ indexing â†’ demo query
python main.py
```

This executes:

1. [data_preparation.py](data_preparation.py) - Download videos, clean dataset
2. [eda_medvidqa.py](eda_medvidqa.py) - Exploratory data analysis
3. [multimodal_pipeline_with_sliding_window.py](multimodal_pipeline_with_sliding_window.py) - Extract features, build indices
4. [query_faiss.py](query_faiss.py) - Demo query with answer generation

### Option 2: Step-by-Step (Recommended for Development)

```bash
# Step 1: Download and prepare data
python data_preparation.py

# Step 2: Exploratory analysis (optional)
python eda_medvidqa.py

# Step 3: Process videos and build indices
python multimodal_pipeline_with_sliding_window.py

# Step 4: Query the system
python query_faiss.py --query "How to check for mouth cancer at home?" --split test --generate_answer --enable_curation
```

### Option 3: Quick Demo (Skip Video Processing)

If you already have FAISS indices:

```bash
python query_faiss.py \
  --query "How to do CPR on an infant?" \
  --split test \
  --hybrid \
  --alpha 0.3 \
  --enable_curation \
  --enable_attribution \
  --generate_answer
```

## ğŸ“– Detailed Usage Guide

### 1. Data Preparation

**Script**: [data_preparation.py](data_preparation.py)

**What it does**:

- Downloads videos from YouTube using metadata in [MedVidQA](MedVidQA)
- Cleans dataset (removes duplicates, validates timestamps)
- Filters by successfully extracted features
- Organizes into [videos_train](videos_train), [videos_val](videos_val), [videos_test](videos_test)

**Usage**:

```bash
# Standard run
python data_preparation.py

# Custom options (edit script variables)
MODEL_NAME = "openai/whisper-tiny"  # ASR model for filtering
```

**Output**:

- [videos_train](videos_train) - Training videos (800 videos)
- [videos_val](videos_val) - Validation videos (49 videos)
- [videos_test](videos_test) - Test videos (50 videos)
- [MedVidQA_cleaned](MedVidQA_cleaned) - Cleaned JSON metadata

---

### 2. Exploratory Data Analysis

**Script**: [eda_medvidqa.py](eda_medvidqa.py)

**What it does**:

- Analyzes dataset statistics (video length distribution, question types)
- Visualizes temporal patterns (answer duration, coverage)
- Generates plots saved to `EDA/`

**Usage**:

```bash
python eda_medvidqa.py
```

**Output**:

- Console statistics (video counts, average durations)
- Plots in `EDA/`:
  - Video length distribution
  - Answer duration distribution
  - Question length histogram
  - Temporal coverage analysis

---

### 3. Multimodal Feature Extraction

**Script**: [multimodal_pipeline_with_sliding_window.py](multimodal_pipeline_with_sliding_window.py)

**What it does**:

- **Phase 2**: ASR transcription â†’ Text/visual embedding extraction
- **Phase 3**: FAISS index construction

**Architecture**:

- Uses [VideoProcessor](video_processing/pipeline.py) for orchestration
- Modular components in [video_processing](video_processing):
  - [video_processing/asr.py](video_processing/asr.py) - Whisper ASR
  - [video_processing/text_embeddings.py](video_processing/text_embeddings.py) - Sliding window + BiomedCLIP text
  - [video_processing/visual_embeddings.py](video_processing/visual_embeddings.py) - Adaptive frame sampling + BiomedCLIP vision

**Usage**:

**Default Configuration** (recommended):

```bash
python multimodal_pipeline_with_sliding_window.py
```

**Custom Hyperparameters**:

```python
# Edit main() function in the script
process_split(
    split="train",
    video_dir="videos_train",
    text_feat_dir="feature_extraction/textual/train",
    visual_feat_dir="feature_extraction/visual/train",
    faiss_text_path="faiss_db/textual_train.index",
    faiss_visual_path="faiss_db/visual_train.index",

    # Text embedding parameters
    window_size=256,              # Token window size
    stride=192,                   # Overlap (192 = 75% overlap)
    min_coverage_contribution=0.05,  # Min new coverage to keep window
    deduplication_mode='coverage',   # 'coverage', 'similarity', 'aggressive', 'none'

    # Visual embedding parameters
    frames_per_segment=2,         # Frames extracted per segment
    sampling_strategy='adaptive',  # 'uniform', 'adaptive', 'quality_based'
    quality_filter=False,         # Enable frame quality filtering
    aggregation_method='mean'     # 'mean' or 'max' for multi-frame pooling
)
```

**Parallel Processing**:

```python
# Edit at top of script
ENABLE_PARALLEL = True  # Enable/disable multiprocessing
batch_size = 4          # Videos per batch (adjust for RAM)
max_workers = 2         # CPU/GPU workers (adjust for hardware)
```

**Output**:

- `feature_extraction/textual/{split}/` - Text embeddings JSON
- `feature_extraction/visual/{split}/` - Visual embeddings JSON
- `faiss_db/textual_{split}.index` - Text FAISS index
- `faiss_db/visual_{split}.index` - Visual FAISS index
- `faiss_db/*.meta.json` - Metadata files

**Performance Tips**:

- **GPU acceleration**: Automatically uses CUDA/MPS if available
- **Memory management**: Adjust `batch_size` if running out of RAM
- **Skip processed videos**: Set `skip_if_exists=True` (default)

---

### 4. FAISS Index Management

**Script**: [embedding_storage.py](embedding_storage.py)

**What it does**:

- Builds FAISS indices from JSON feature files
- Normalizes embeddings (L2 norm)
- Persists metadata alongside indices

**Usage**:

**Build indices for a split**:

```bash
# Build both text and visual indices
python embedding_storage.py --split train

# Build only text index
python embedding_storage.py --split train --modality text

# Build only visual index
python embedding_storage.py --split train --modality visual
```

**Custom directories**:

```bash
python embedding_storage.py \
  --split train \
  --feature_dir my_features/ \
  --output_dir my_indices/
```

**When to use**:

- After manual feature extraction
- To rebuild indices without reprocessing videos
- When experimenting with index configurations

---

### 5. Querying the System

**Script**: [query_faiss.py](query_faiss.py)

**What it does**:

- Embeds query with BiomedCLIP
- Searches FAISS indices (text + visual)
- Applies hybrid search (dense + BM25)
- Aggregates multimodal results
- Generates answers with GPT-4o-mini
- Evaluates against ground truth (if provided)

**Basic Usage**:

**Simple query** (dense search only):

```bash
python query_faiss.py \
  --query "How to perform CPR on an infant?" \
  --split test \
  --final_k 10
```

**Hybrid search** (recommended):

```bash
python query_faiss.py \
  --query "How to do a mouth cancer check at home?" \
  --split test \
  --hybrid \
  --alpha 0.3 \
  --fusion linear \
  --final_k 10
```

**With answer generation**:

```bash
python query_faiss.py \
  --query "How to do a mouth cancer check at home?" \
  --split test \
  --hybrid \
  --enable_curation \
  --enable_attribution \
  --generate_answer
```

**Full evaluation with ground truth**:

```bash
python query_faiss.py \
  --query "How to do a mouth cancer check at home?" \
  --split test \
  --hybrid \
  --eval \
  --video_id "_6csIJAWj_s" \
  --answer_start 35 \
  --answer_end 96 \
  --generate_answer \
  --enable_curation
```

**Advanced Options**:

**Search Configuration**:

```bash
--local_k 50          # Results per index before merging (default: 50)
--final_k 10          # Final combined results (default: 10)
--text_weight 0.6     # Text importance in multimodal fusion (default: 0.6)
--visual_weight 0.4   # Visual importance (default: 0.4)
```

**Hybrid Search**:

```bash
--hybrid              # Enable hybrid search
--alpha 0.3           # Dense weight (0-1, default: 0.3)
                      # 0.3 = 70% BM25 + 30% dense (optimal for medical)
--fusion linear       # 'linear' (score-based) or 'rrf' (rank-based)
--expand_query        # Medical term expansion in BM25 (default: enabled)
--analyze_fusion      # Show BM25 vs dense contribution breakdown
```

**Context Curation**:

```bash
--enable_curation           # Enable adaptive context selection
--quality_threshold 0.1     # Min quality score (default: 0.1)
--token_budget 600          # Max context tokens (default: 600)
--nli_top_k 15             # Candidates for NLI scoring (default: 15)
```

**Answer Generation**:

```bash
--generate_answer              # Enable LLM answer generation
--answer_model gpt-4o-mini    # Model (default: gpt-4o-mini)
--answer_max_tokens 250        # Max output length (default: 250)
--answer_temperature 0.3       # Sampling temp (default: 0.3)
--enable_attribution           # Self-reflection attribution
```

**Aggregation Modes**:

```bash
--mode segment        # Segment-level multimodal linking (default)
--mode video          # Video-level aggregation
--top_videos 5        # Videos in video mode (default: 5)
--hierarchical        # Extended context with overlapping timestamps
```

**Output**:

- Console: Search results, evaluation metrics, generated answer
- JSON: `multimodal_search_results_*.json` (search results + answer)

---

### 6. Batch Evaluation

**Script**: [run_full_evaluation.py](run_full_evaluation.py)

**What it does**:

- Evaluates entire dataset splits
- Computes retrieval and generation metrics
- Generates comprehensive reports
- Tracks performance and costs

**Usage**:

**Evaluate test split**:

```bash
python run_full_evaluation.py \
  --split test \
  --enable_curation \
  --enable_attribution
```

**Evaluate all splits** (train + val + test):

```bash
python run_full_evaluation.py \
  --split all \
  --alpha 0.3 \
  --quality_threshold 0.1
```

**Quick test** (first 10 queries):

```bash
python run_full_evaluation.py \
  --split test \
  --max_queries 10
```

**Custom configuration**:

```bash
python run_full_evaluation.py \
  --split test \
  --alpha 0.3 \
  --local_k 50 \
  --final_k 10 \
  --quality_threshold 0.1 \
  --token_budget 600 \
  --enable_curation \
  --enable_attribution
```

**Output**:

- `evaluation_results/` directory:
  - `{split}_results_{timestamp}.json` - Detailed per-query results
  - `{split}_summary_{timestamp}.json` - Aggregated statistics
  - `{split}_report_{timestamp}.md` - Human-readable report

**Metrics Reported**:

**Retrieval**:

- Recall@5, Recall@10
- MRR (Mean Reciprocal Rank)
- Temporal IoU, Precision, Recall, F1
- Video hit rate
- Mean temporal distance

**Generation**:

- Answer quality (length, coherence)
- Confidence scores
- Token usage and costs

**Performance**:

- Search latency
- Generation latency
- End-to-end timing

---

### 7. Hyperparameter Tuning

**Script**: [hyperparameter_tuning.py](hyperparameter_tuning.py)

**What it does**:

- Grid search over embedding parameters
- Evaluates on metrics (coverage, relevance, speed)
- Generates comparison plots

**Usage**:

```bash
python hyperparameter_tuning.py
```

**Customize grid** (edit script):

```python
HYPERPARAMETER_GRID = {
    # Text parameters
    'window_size': [128, 256, 512],
    'stride': [96, 192, 384],
    'deduplication_mode': ['coverage', 'similarity', 'none'],

    # Visual parameters
    'frames_per_segment': [1, 2, 4],
    'sampling_strategy': ['uniform', 'adaptive', 'quality_based'],
    'aggregation_method': ['mean', 'max']
}
```

**Output**:

- `hyperparameter_tuning_results.json` - Detailed results
- `hyperparameter_comparison.png` - Visualization

---

### 8. Specialized Tools

#### 8.1 Rebuild Visual Embeddings

**Script**: [rebuild_visual_embeddings.py](rebuild_visual_embeddings.py)

**When to use**: Update visual features without reprocessing text

```bash
python rebuild_visual_embeddings.py --split train
```

#### 8.2 Compare Search Methods

**Script**: [compare_search_methods.py](compare_search_methods.py)

**What it does**: A/B test dense vs hybrid search

```bash
python compare_search_methods.py \
  --query "How to do a mouth cancer check at home?" \
  --expected_video "_6csIJAWj_s" \
  --top_k 10
```

#### 8.3 Answer Evaluation

**Script**: [evaluation.py](evaluation.py)

**What it does**: Standalone answer quality assessment

```python
from evaluation import AnswerEvaluator

evaluator = AnswerEvaluator()
result = evaluator.evaluate_answer(
    generated_answer="...",
    ground_truth="...",
    query="..."
)
```

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data_preparation.py                 # Download videos, clean dataset
â”œâ”€â”€ eda_medvidqa.py                    # Exploratory data analysis
â”œâ”€â”€ multimodal_pipeline_with_sliding_window.py  # Feature extraction pipeline
â”œâ”€â”€ embedding_storage.py                # FAISS index management
â”œâ”€â”€ query_faiss.py                     # Query interface (main entry point)
â”œâ”€â”€ run_full_evaluation.py             # Batch evaluation
â”œâ”€â”€ hyperparameter_tuning.py           # Hyperparameter optimization
â”œâ”€â”€ compare_search_methods.py          # Search method comparison
â”œâ”€â”€ evaluation.py                      # Answer evaluation metrics
â”œâ”€â”€ rebuild_visual_embeddings.py       # Visual embedding rebuild
â”œâ”€â”€ main.py                            # Full pipeline orchestrator
â”‚
â”œâ”€â”€ video_processing/                  # Modular video processing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ asr.py                        # Whisper ASR
â”‚   â”œâ”€â”€ text_embeddings.py            # Sliding window + BiomedCLIP text
â”‚   â”œâ”€â”€ visual_embeddings.py          # Frame sampling + BiomedCLIP vision
â”‚   â”œâ”€â”€ deduplication.py              # Embedding deduplication
â”‚   â””â”€â”€ pipeline.py                   # VideoProcessor orchestrator
â”‚
â”œâ”€â”€ search/                            # Search components
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dense_search.py               # FAISS + embedding models
â”‚   â”œâ”€â”€ sparse_search.py              # BM25 + query expansion
â”‚   â”œâ”€â”€ hybrid_fusion.py              # Hybrid search engine
â”‚   â”œâ”€â”€ aggregation.py                # Multimodal result merging
â”‚   â”œâ”€â”€ hierarchical_search.py        # Extended context retrieval
â”‚   â””â”€â”€ utils.py                      # Helper functions
â”‚
â”œâ”€â”€ generation/                        # Answer generation
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ answer_generator.py           # GPT-4o-mini interface
â”‚   â”œâ”€â”€ context_curator.py            # Adaptive context selection
â”‚   â”œâ”€â”€ attribution.py                # Self-reflection attribution
â”‚   â””â”€â”€ prompts.py                    # Prompt templates
â”‚
â”œâ”€â”€ MedVidQA/                         # Original dataset
â”‚   â”œâ”€â”€ train.json
â”‚   â”œâ”€â”€ val.json
â”‚   â”œâ”€â”€ test.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ MedVidQA_cleaned/                 # Cleaned dataset
â”‚   â”œâ”€â”€ train_openai_whisper_tiny_cleaned.json
â”‚   â”œâ”€â”€ val_openai_whisper_tiny_cleaned.json
â”‚   â””â”€â”€ test_openai_whisper_tiny_cleaned.json
â”‚
â”œâ”€â”€ videos_train/                     # Training videos (800 videos)
â”œâ”€â”€ videos_val/                       # Validation videos (49 videos)
â”œâ”€â”€ videos_test/                      # Test videos (50 videos)
â”‚
â”œâ”€â”€ feature_extraction/               # Extracted embeddings
â”‚   â”œâ”€â”€ textual/
â”‚   â”‚   â”œâ”€â”€ train/
â”‚   â”‚   â”œâ”€â”€ val/
â”‚   â”‚   â””â”€â”€ test/
â”‚   â””â”€â”€ visual/
â”‚       â”œâ”€â”€ train/
â”‚       â”œâ”€â”€ val/
â”‚       â””â”€â”€ test/
â”‚
â”œâ”€â”€ faiss_db/                         # FAISS indices
â”‚   â”œâ”€â”€ textual_train.index
â”‚   â”œâ”€â”€ textual_train.index.meta.json
â”‚   â”œâ”€â”€ textual_val.index
â”‚   â”œâ”€â”€ textual_val.index.meta.json
â”‚   â”œâ”€â”€ textual_test.index
â”‚   â”œâ”€â”€ textual_test.index.meta.json
â”‚   â”œâ”€â”€ visual_train.index
â”‚   â”œâ”€â”€ visual_train.index.meta.json
â”‚   â”œâ”€â”€ visual_val.index
â”‚   â”œâ”€â”€ visual_val.index.meta.json
â”‚   â”œâ”€â”€ visual_test.index
â”‚   â””â”€â”€ visual_test.index.meta.json
â”‚
â”œâ”€â”€ evaluation_results/               # Evaluation outputs
â”‚   â”œâ”€â”€ test_results_*.json
â”‚   â”œâ”€â”€ test_summary_*.json
â”‚   â””â”€â”€ test_report_*.md
|
â”œâ”€â”€ EDA/                              # Contains visualizations
|
â”œâ”€â”€ images/                           # Phase wise and system architecture
â”‚   â”œâ”€â”€ phase_1.png
â”‚   â”œâ”€â”€ phase_2.png
â”‚   â”œâ”€â”€ phase_3.png
â”‚   â”œâ”€â”€ phase_4.png
â”‚   â”œâ”€â”€ phase_5.png
â”‚   â”œâ”€â”€ System_Architecture.png
â”‚
â”œâ”€â”€ requirements.txt                  # Core dependencies
â”œâ”€â”€ .env                              # API keys (create this)
â”œâ”€â”€ .gitignore
```

---

## ğŸ”§ Configuration Options

### Environment Variables (`.env`)

```bash
# Required
OPENAI_API_KEY=sk-...                 # OpenAI API key

# Optional
HF_TOKEN=hf_...                       # HuggingFace token (for BiomedCLIP)
```

### Common Hyperparameters

**Text Embeddings**:

- `window_size=256` - Token window size
- `stride=192` - Overlap (75% overlap = stride/window)
- `min_coverage_contribution=0.05` - Min new coverage threshold
- `deduplication_mode='coverage'` - 'coverage', 'similarity', 'aggressive', 'none'

**Visual Embeddings**:

- `frames_per_segment=2` - Frames extracted per segment
- `sampling_strategy='adaptive'` - 'uniform', 'adaptive', 'quality_based'
- `quality_filter=False` - Enable sharpness-based filtering
- `aggregation_method='mean'` - 'mean' or 'max' for frame pooling

**Search**:

- `alpha=0.3` - Dense weight in hybrid search (0.3 = 70% BM25 + 30% dense)
- `fusion='linear'` - 'linear' (score-based) or 'rrf' (rank-based)
- `local_k=50` - Results per index
- `final_k=10` - Final combined results

**Context Curation**:

- `quality_threshold=0.1` - Min relevance score
- `token_budget=600` - Max context tokens
- `nli_top_k=15` - Candidates for NLI scoring

**Answer Generation**:

- `answer_model='gpt-4o-mini'` - LLM model
- `answer_max_tokens=250` - Max answer length (~200 words)
- `answer_temperature=0.3` - Lower = more deterministic

---

## ğŸ¯ Example Workflows

### Workflow 1: First-Time Setup

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Configure environment
echo "OPENAI_API_KEY=sk-..." > .env

# 3. Download and prepare data
python data_preparation.py

# 4. Process videos (this takes time!)
python multimodal_pipeline_with_sliding_window.py

# 5. Test with a query
python query_faiss.py \
  --query "How to do a mouth cancer check at home?" \
  --split train \
  --hybrid \
  --generate_answer
```

### Workflow 2: Development/Experimentation

```bash
# Test on a single video
python -c "
from multimodal_pipeline_with_sliding_window import test_single_video
test_single_video(
    'videos_test/sample.mp4',
    'feature_extraction/textual/test_single',
    'feature_extraction/visual/test_single'
)
"

# Query with different fusion strategies
python query_faiss.py --query "..." --split test --hybrid --alpha 0.1  # More BM25
python query_faiss.py --query "..." --split test --hybrid --alpha 0.5  # Balanced
python query_faiss.py --query "..." --split test --hybrid --alpha 0.9  # More dense

# Compare search methods
python compare_search_methods.py --query "..." --expected_video "..."
```

### Workflow 3: Batch Evaluation

```bash
# Evaluate with different configurations
python run_full_evaluation.py --split test --alpha 0.1
python run_full_evaluation.py --split test --alpha 0.3
python run_full_evaluation.py --split test --alpha 0.5

# Compare results
diff evaluation_results/test_summary_*_alpha0.1.json \
     evaluation_results/test_summary_*_alpha0.3.json
```

### Workflow 4: Hyperparameter Optimization

```bash
# 1. Define grid in hyperparameter_tuning.py
# 2. Run grid search
python hyperparameter_tuning.py

# 3. Analyze results
# Check hyperparameter_tuning_results.json
# View hyperparameter_comparison.png

# 4. Update pipeline with best hyperparameters
# Edit multimodal_pipeline_with_sliding_window.py
```

---

## ğŸ“Š Performance Benchmarks

### Hardware Performance (899 videos)

| Hardware               | Feature Extraction Time | Index Build Time | Query Latency |
| ---------------------- | ----------------------- | ---------------- | ------------- |
| CPU Only (16 cores)    | ~36-48 hours            | ~5 min           | ~1-2s         |
| NVIDIA RTX 3080 (10GB) | ~8-10 hours             | ~5 min           | ~0.5-1s       |
| Apple M1 Max (64GB)    | ~10-12 hours            | ~5 min           | ~0.5-1s       |

### Retrieval Performance (Test Set, 155 queries)

| Configuration  | Recall@5  | MRR       | Temporal F1 | Query Cost |
| -------------- | --------- | --------- | ----------- | ---------- |
| Dense only     | 0.742     | 0.513     | 0.458       | $0.0003    |
| Hybrid (Î±=0.3) | **0.789** | **0.567** | **0.512**   | $0.0003    |
| BM25 only      | 0.681     | 0.429     | 0.391       | $0.0003    |

### Answer Quality (GPT-4o-mini)

| Metric                | Mean     | Std      |
| --------------------- | -------- | -------- |
| ROUGE-L               | 0.387    | 0.142    |
| Confidence            | 0.783    | 0.118    |
| Answer Length (words) | 187.3    | 42.6     |
| Cost per Query        | $0.00042 | $0.00013 |

---

## ğŸ› Troubleshooting

### Common Issues

**1. CUDA Out of Memory**

```python
# Reduce batch size in multimodal_pipeline_with_sliding_window.py
batch_size = 2  # Default: 4
```

**2. FAISS Index Not Found**

```bash
# Rebuild indices
python embedding_storage.py --split train --modality both
```

**3. OpenAI API Rate Limit**

```python
# Add delay in run_full_evaluation.py
import time
time.sleep(1)  # Between queries
```

**4. YouTube Download Fails**

```bash
# Update youtube-po-token-generator
npm update -g youtube-po-token-generator

# Or use pytubefix with authentication
# See: https://github.com/JuanBindez/pytubefix
```

**5. SciSpacy Model Not Found**

```bash
# Reinstall SciSpacy model
pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.5.3/en_core_sci_sm-0.5.3.tar.gz
```

## ğŸ“„ License

This project is for research and educational purposes. Video content is subject to original YouTube creators' licenses.

---

## ğŸ¤ Contributing

Contributions welcome! Areas for improvement:

- Support for additional embedding models (e.g., PubMedBERT)
- Alternative LLMs (Claude, Gemini, LLaMA)
- Advanced fusion strategies
- Multi-hop reasoning
- Interactive UI/API

**Built with â¤ï¸ for the medical AI research community**
